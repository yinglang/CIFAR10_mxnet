{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.021724Z",
     "start_time": "2017-11-05T03:46:11.585655Z"
    },
    "collapsed": true
   },
   "source": [
    "# 1. import needed package and set global env\n",
    "\n",
    "you need to download CIFAR10 dataset from kaggle and then extract to \\${data_dir}, (such as \"/root/Workspace/data/CIFAR10_kaggle/\")<br/>\n",
    "\\${data_dir} should like this(dir \"train_valid_test\" will generate in data prepared model):</br>\n",
    "![](../../_image/data_dir.png)<br/>\n",
    "and then set variable\n",
    "```\n",
    "set var data_dir=${data_dir}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "from netlib import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ctx = mx.gpu(6)\n",
    "\n",
    "data_dir = \"/root/Workspace/data/CIFAR10_kaggle/\"\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "label_file = \"trainLabels.csv\"\n",
    "input_dir = 'train_valid_test/'\n",
    "valid_ratio = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "force_recreate_dir = False\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data prepared(need not run this block repeatly)\n",
    "if you have do dataset prepared and generate 'train_valid_test' dir to orgnize data, just ignore this block(that mean you need not run this model and just skip it)<br/>\n",
    "\n",
    "the step may cost about 40~60min.<br/>\n",
    "\n",
    "so to avoid run this model repeatly, we check the created dir, and if exist will not create again, if you want to recreate them, set force_recreate_dir=True.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.087726Z",
     "start_time": "2017-11-05T03:46:12.036394Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_valid_test dir has been created and if you want to recreate them set force_recreate=True(normally you needn't to do it.)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data prepare, reference to http://zh.gluon.ai/chapter_computer-vision/kaggle-gluon-cifar10.html\n",
    "\"\"\"\n",
    "def reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict((int(idx), label)for idx, label in tokens)\n",
    "    labels = set(idx_label.values())\n",
    "    print labels\n",
    "    \n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    num_train_tuning = int(num_train * (1 - valid_ratio))\n",
    "    assert 0 < num_train_tuning < num_train\n",
    "    num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "    label_count = dict()\n",
    "    \n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "    \n",
    "    # copy to create train set and valid set and train_valid set (train/label, valid/label, train_valid/label)\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = int(train_file.split('.')[0])\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                  os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label_count.get(label, 0) < num_train_tuning_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'valid', label))\n",
    "    \n",
    "    # copy to create test set (test/unkown)\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unkown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file), \n",
    "                   os.path.join(data_dir, input_dir, 'test', 'unkown'))\n",
    "\n",
    "\n",
    "if force_recreate_dir==False and os.path.exists(data_dir + input_dir) and os.path.exists(data_dir + input_dir + train_dir) and os.path.exists(data_dir + input_dir + test_dir):\n",
    "    print \"train_valid_test dir has been created and if you want to recreate them\",\n",
    "    print \"set force_recreate=True(normally you needn't to do it.)\"\n",
    "else:\n",
    "    reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data loader, data argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.804734Z",
     "start_time": "2017-11-05T03:46:12.785031Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + '/train', flag=1, transform=transform_train)\n",
    "    valid_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + '/valid', flag=1, transform=transform_test)\n",
    "    train_valid_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + '/train_valid', flag=1, transform=transform_train)\n",
    "    test_ds = vision.ImageFolderDataset(data_dir + \"/\" + input_dir + \"/test\", flag=1, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')\n",
    "    return train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:14.408716Z",
     "start_time": "2017-11-05T03:46:14.361070Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 mixup\n",
    "1. mixup define\n",
    "2. mixup test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mixup(x1, y1, x2, y2, alpha):\n",
    "    lam = np.random.beta(alpha)\n",
    "    x = lam * x1 + (1 - lam) * x2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.model_zoo import vision\n",
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "mixup_train_data, _, _, _, _, _ = data_loader(batch_size, trainform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:15.590464Z",
     "start_time": "2017-11-05T03:46:15.513162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss(), \n",
    "          mixup_train_data=None, mixup_alpha=0.2):\n",
    "    def train_batch(data, label):\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(ctx))\n",
    "            loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        _loss = nd.mean(loss).asscalar()\n",
    "        _acc = utils.accuracy(output, label)\n",
    "        train_loss += _loss\n",
    "        train_acc += _acc\n",
    "\n",
    "        if verbose:\n",
    "            print \" # iter\", i,\n",
    "            print \"loss %.5f\" % _loss, \"acc %.5f\" % _acc,\n",
    "            print \"w (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.data()),\n",
    "            print \") g (\",\n",
    "            for k in w_key:\n",
    "                w = net.collect_params()[k]\n",
    "                print \"%.5f, \" % abs_mean(w.grad()),\n",
    "            print \")\"\n",
    "            i += 1\n",
    "            \n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "            \n",
    "        if mixup_train_data is None:\n",
    "            for data, label in train_data:\n",
    "                train_batch(data, label)\n",
    "        else:\n",
    "            for (x1, y1), (x2, y2) in zip(train_data, mixup_train_data):\n",
    "                data, label = mixup(x1, y1, x2, y2, mixup_alpha)\n",
    "                train_batch(data, label)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. get net and do EXP\n",
    "the model define in netlib.py, just invoke to create it, and not use any pretrain model<br/>\n",
    "\n",
    "#### note: if you just want to got a submission result, just ignore Exp1~5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp1: res164_v2 + DA1: 0.9529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [90, 140]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"models/shelock_resnet_orign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp2:res164_v2 + DA2: 0.9527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA2\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train2)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/resnet164_e300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp3: res164_v2 + focal loss + DA3: 0.9540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "tranform_train = transform_train_DA3\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = FocalLoss()\n",
    "\n",
    "num_epochs = 255\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/res164__2_e255_focal_clip_all_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp4: res164_v2 + focal loss + DA3 + only train_data: 0.9506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA3\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = FocalLoss()\n",
    "\n",
    "num_epochs = 255\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/resnet164_e0-255_focal_clip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp5: sherlock_densenet: 0.9539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [90, 140]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.hybridize()\n",
    "net.initialize(ctx=ctx)\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/shelock_densenet_orign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. merge result\n",
    "to run this two block, you need at least five model params file, name as follow in model dir.\n",
    "```\n",
    "model_list = ['res164__2_e255_focal_clip_all_data', 'resnet164_e300', 'resnet164_e0-255_focal_clip',\n",
    "              'shelock_densenet_orign', 'shelock_resnet_orign']\n",
    "```\n",
    "you can train from **5. get net and do EXP**, or you can download them from\n",
    "```\n",
    "link: https://pan.baidu.com/s/1pLjzQWj key: f6p3\n",
    "```\n",
    "you can only download this five model param file in model dir, and then run first block to generate middle result in result dir.<br/>\n",
    "or you can download all 8 model param file in models and the middle result file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:21.596302Z",
     "start_time": "2017-11-05T03:46:21.241490Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "generate CIFAR10 output result\n",
    "\"\"\"\n",
    "mkdir_if_not_exist('result/')\n",
    "\n",
    "def save_net_result(net, filename, test_data, ctx):\n",
    "    output = nd.zeros(shape=(300000, 10), ctx=ctx)\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        output[i*batch_size:i*batch_size+data.shape[0],:] = net(data.as_in_context(ctx))\n",
    "    nd.save(filename, output)\n",
    "\n",
    "def test_net(data):\n",
    "    return data.reshape((data.shape[0], -1))[:, :10]\n",
    "\n",
    "def save_model_result(model_name, ctx):\n",
    "    net.load_params(\"models/\" + model_name, ctx=ctx)\n",
    "    save_net_result(net, \"result/\" + model_name, test_data, ctx)\n",
    "\n",
    "model_list = ['resnet164_e255_focal_clip', 'res164__2_e255_focal_clip_all_data', 'resnet164_e300','resnet164_e0-255_focal_clip',\n",
    "              'res18_9', \n",
    "              'log_shelock_densenet', 'shelock_densenet_orign',\n",
    "              'shelock_resnet_orign']\n",
    "weight_list = [0.9535, 0.9540, 0.95270, 0.95, 0.93230, 0.9346, 0.9539, 0.95]\n",
    "\n",
    "net = ResNet164_v2(10)\n",
    "for model_name in model_list[:4]:\n",
    "    if not os.path.exists(\"models/\" + model_name): continue\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = ResNet(10)\n",
    "for model_name in model_list[4:5]:\n",
    "    if not os.path.exists(\"models/\" + model_name): continue\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "for model_name in model_list[5:7]:\n",
    "    if not os.path.exists(\"models/\" + model_name): continue\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = ResNet164_v2(10)\n",
    "for model_name in model_list[7:]:\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-05T03:47:12.464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classfiy test set from generated result\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(128, transform_train_DA1)\n",
    "\n",
    "def mesuare_sum(preds, weight_list=None):\n",
    "    if weight_list is None:\n",
    "        weight_list = [1] * len(preds)\n",
    "    output = preds[0] * weight_list[0]\n",
    "    for i in range(1, len(preds)):\n",
    "        output = output + preds[i] * weight_list[i]\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "def mesuare_softmax_sum(preds, weight_list=None):\n",
    "    if weight_list is None:\n",
    "        weight_list = [1] * len(preds)\n",
    "    output = nd.softmax(preds[0], axis=1) * weight_list[0]\n",
    "    for i in range(1, len(preds)):\n",
    "        output = output + nd.softmax(preds[i], axis=1) * weight_list[i]\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "def mesuare_biggest(preds, weight_list=None):\n",
    "    if weight_list is not None:\n",
    "        for i in range(len(preds)):\n",
    "            preds[i] = preds[i] * weight_list[i]\n",
    "    output = nd.concat(*preds, dim=1)\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "model_list = ['res164__2_e255_focal_clip_all_data', 'resnet164_e300', 'resnet164_e0-255_focal_clip',\n",
    "              'shelock_densenet_orign', 'shelock_resnet_orign']\n",
    "weight_list = [0.9540, 0.95270, 0.95, 0.9539, 0.95]\n",
    "#weight_list=None\n",
    "\n",
    "preds = []\n",
    "for result_name in model_list:\n",
    "    preds.append(nd.load(\"result/\"+result_name)[0].as_in_context(ctx))\n",
    "\n",
    "#preds = mesuare_biggest(preds, weight_list)\n",
    "preds = mesuare_sum(preds, weight_list)\n",
    "#preds = mesuare_softmax_sum(preds, weight_list)\n",
    "\n",
    "sorted_ids = list(range(1, 300000 + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission/concat_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
